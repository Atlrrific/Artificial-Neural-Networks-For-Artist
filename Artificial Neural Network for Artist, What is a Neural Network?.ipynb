{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an Artificial Neural Network:\n",
    "An artificial neural network is a computational model that loosely resembles the way our brain works.\n",
    "\n",
    "As far as we know, our brain is made up of neurons that get \"triggered\" depending on the information we capture with in our senses. For example, whenever we heard the voice of somebody we know our brain processes the sound information  and classify it to match the person that we are familiar with. \n",
    "\n",
    "In the Artificial Neural Network world we have artificial neurons that get triggered or activated given some numerical value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron\n",
    "The Perceptron might be an example of the simplest artificial neuron that we can possible represent.\n",
    "It takes in an input and gets activated if we have reached certain threshold( $\\theta$).\n",
    "The value that goes into the perceptron is composed of a sum of the product of weights and input parameters. \n",
    "\n",
    "\n",
    "$$\n",
    "  PERCEPTRON : f(x) =  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & \\sum\\limits_{inputs}  w_i * x_i >  \\theta \\\\\n",
    "      0 & otherwise \\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "$$\n",
    "\n",
    "Which can be translated to: \n",
    "$$\n",
    "  PERCEPTRON : f(x) =  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & \\sum\\limits_{inputs}  w_i * x_i - \\theta >  0 \\\\\n",
    "      0 & otherwise \\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "$$\n",
    "\n",
    "#### The Weights\n",
    "These weights are the part of a neural network that is dynamic and learns from examples. Changes in this weights get adapted in order to optimize a desire output.\n",
    "In addition, the threshold  $\\theta$  gets adapted in order to get a desired outcome.\n",
    "\n",
    "#### The Theta term or Bias\n",
    "The theta term on the equation is called the bias term. Normally when neural networks get trained it is the first term that learns features on the input since it is always active and positive. We will discuss a little more about the Bias on more complex neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSJEmSJEmSJEmSNCH9P3LvzQZ7yI3qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dd5c110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.arange(1, 14, 1)\n",
    "y = [0]*(int(len(x)/2))\n",
    "y+= [1]*(int(len(x)/2)+1)\n",
    "plt.step(x, y, label='pre (default)')\n",
    "plt.ylim(-1, 2)\n",
    "plt.xlim(2, 10)\n",
    "plt.title(\"Perceptron Function with theta = 6\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Learning Rule\n",
    "In order for the perceptron to output the correct value it has to learn from previous examples. \n",
    "Learning happens when a perceptron makes a mistake. In this case the perceptron adapts its parameters in order to output the correct result given its inputs. \n",
    "The weights get updated and the threshold might change. \n",
    "\n",
    "The learning rule for the weights is defined as:\n",
    "\n",
    "$$ w_1(t+1) = w_i(t) + \\alpha * (teacher - output) *x_i $$\n",
    "\n",
    "Where the teacher is the expected result and output is the current output of the neural network. \n",
    "If the neural network makes a mistake the difference of the teacher and output will inflict some change on a particular weight on the neural network , otherwise the network will stay the same.\n",
    "\n",
    "The learning rate $\\alpha$ defines the amount of change that is inflicted over a weight when it makes a mistake. \n",
    "\n",
    "### AND Gate Example\n",
    "Lets consider an AND logical gate.\n",
    "\n",
    "| X   | Y  | Output |\n",
    "| ---- |:----:| ----:|\n",
    "| 0   | 0 | 0 |\n",
    "| 1   | 0 | 0 |\n",
    "| 0   | 1 | 0 |\n",
    "| 1   | 1 | 1 |\n",
    "\n",
    "It is only one if both inputs are one, otherwise the output is zero. \n",
    "\n",
    "We can make a model of an AND function using a perceptron neuron. \n",
    "Recall the perceptron function, it takes into account the weighted sum of an input which in this case is the X and Y binary inputs. \n",
    "We can initialize the weights to zero. \n",
    "\n",
    "So the propagation of the inputs to the output of this neural network will be:\n",
    "\n",
    "$$\n",
    "  output =  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & \\sum\\limits_{inputs} = w_i * x_i >  \\theta \\\\\n",
    "      0 & otherwise \\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "$$\n",
    "\n",
    "Which is equal to:\n",
    "\n",
    "$$\n",
    "  output =  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & if: w_x * x +  w_y * y  - \\theta >  0 \\\\\n",
    "      0 & otherwise \\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "$$\n",
    "\n",
    "$\\theta $ is the bias term since it is always on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|X_2 \t| X_1 \t| T \t| W_1 \t| W_2\t|delta \t|\n",
      "|Epoch: 0 \t|\t|\t|\t|\t|\n",
      "| 0 \t| 0 \t| 0 \t| 0.0 \t| 0.0 \t| -1 \t|\n",
      "| 0 \t| 1 \t| 0 \t| 0.0 \t| 0.0 \t| 0 \t|\n",
      "| 1 \t| 0 \t| 0 \t| 0.0 \t| 0.0 \t| 0 \t|\n",
      "| 1 \t| 1 \t| 1 \t| 0.5 \t| 0.5 \t| 1 \t|\n",
      "|Epoch: 1 \t|\t|\t|\t|\t|\n",
      "| 0 \t| 0 \t| 0 \t| 0.5 \t| 0.5 \t| -1 \t|\n",
      "| 0 \t| 1 \t| 0 \t| 0.5 \t| 0.0 \t| -1 \t|\n",
      "| 1 \t| 0 \t| 0 \t| 0.5 \t| 0.0 \t| 0 \t|\n",
      "| 1 \t| 1 \t| 1 \t| 1.0 \t| 0.5 \t| 1 \t|\n",
      "|Epoch: 2 \t|\t|\t|\t|\t|\n",
      "| 0 \t| 0 \t| 0 \t| 1.0 \t| 0.5 \t| 0 \t|\n",
      "| 0 \t| 1 \t| 0 \t| 1.0 \t| 0.0 \t| -1 \t|\n",
      "| 1 \t| 0 \t| 0 \t| 0.5 \t| 0.0 \t| -1 \t|\n",
      "| 1 \t| 1 \t| 1 \t| 1.0 \t| 0.5 \t| 1 \t|\n",
      "|Epoch: 3 \t|\t|\t|\t|\t|\n",
      "| 0 \t| 0 \t| 0 \t| 1.0 \t| 0.5 \t| 0 \t|\n",
      "| 0 \t| 1 \t| 0 \t| 1.0 \t| 0.5 \t| 0 \t|\n",
      "| 1 \t| 0 \t| 0 \t| 0.5 \t| 0.5 \t| -1 \t|\n",
      "| 1 \t| 1 \t| 1 \t| 1.0 \t| 1.0 \t| 1 \t|\n",
      "|Epoch: 4 \t|\t|\t|\t|\t|\n",
      "| 0 \t| 0 \t| 0 \t| 1.0 \t| 1.0 \t| 0 \t|\n",
      "| 0 \t| 1 \t| 0 \t| 1.0 \t| 0.5 \t| -1 \t|\n",
      "| 1 \t| 0 \t| 0 \t| 1.0 \t| 0.5 \t| 0 \t|\n",
      "| 1 \t| 1 \t| 1 \t| 1.0 \t| 0.5 \t| 0 \t|\n",
      "|Epoch: 5 \t|\t|\t|\t|\t|\n",
      "| 0 \t| 0 \t| 0 \t| 1.0 \t| 0.5 \t| 0 \t|\n",
      "| 0 \t| 1 \t| 0 \t| 1.0 \t| 0.5 \t| 0 \t|\n",
      "| 1 \t| 0 \t| 0 \t| 1.0 \t| 0.5 \t| 0 \t|\n",
      "| 1 \t| 1 \t| 1 \t| 1.0 \t| 0.5 \t| 0 \t|\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Lets run an AND logical example'''\n",
    "\n",
    "'''Initialize the weights'''\n",
    "w = [0,0]\n",
    "theta = 0\n",
    "alpha = 0.5\n",
    "'''The expected value of the the and gate'''\n",
    "x_labels = [0,0,0,1]\n",
    "\n",
    "'''And gate inputs'''\n",
    "x_values = [(0,0),(0,1),(1,0), (1,1)]\n",
    "\n",
    "print \"|X_2 \\t| X_1 \\t| T \\t| W_1 \\t| W_2\\t|delta \\t|\"\n",
    "\n",
    "'''Setting up the learning rate'''\n",
    "error = True\n",
    "\n",
    "'''Iterating 6 times over the data'''\n",
    "for epoch in range(0,6):\n",
    "    print \"|Epoch:\",epoch,\"\\t|\\t|\\t|\\t|\\t|\"\n",
    "    for index, value in enumerate (x_values):\n",
    "\n",
    "        '''Calculate net output from the perceptron'''\n",
    "        net  = w[0]*value[0] + w[1]*value[1]  \n",
    "        \n",
    "        '''Check if we reached the threshold'''\n",
    "        if(net >= theta): \n",
    "            output = 1 \n",
    "        else: \n",
    "            output = 0\n",
    "\n",
    "        '''Calculating the delta'''\n",
    "        delta = x_labels[index] - output\n",
    "\n",
    "        w[0] = w[0] + alpha*(delta)*value[0]\n",
    "        w[1] = w[1] + alpha*(delta)*value[1]\n",
    "\n",
    "        if delta > 0:\n",
    "            theta -= alpha\n",
    "        elif delta < 0:\n",
    "            theta += alpha\n",
    "\n",
    "        print \"|\",value[0],\"\\t|\",value[1],\"\\t|\",x_labels[index], \"\\t|\",w[0],\"\\t|\", w[1],\"\\t|\", delta,\"\\t|\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we saw in the example everytime the Neural Network did a mistake it updated its weights and bias term. Sometimes these changes balanced out over time, however at the end the was able to learn how to output the correct result form an AND gate given two values. \n",
    "\n",
    "On the next session we will learn about classification and more complex neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
