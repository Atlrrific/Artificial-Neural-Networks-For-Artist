{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using Numpy to Optimize our code\n",
    "\n",
    "### What is numpy?\n",
    "\n",
    "\n",
    "NumPy is the fundamental package for scientific computing with Python. It contains among other things:\n",
    "* a powerful N-dimensional array object\n",
    "* sophisticated (broadcasting) functions\n",
    "* tools for integrating C/C++ and Fortran code\n",
    "* useful linear algebra, Fourier transform, and random number capabilities\n",
    "\n",
    "Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. [1]\n",
    "\n",
    "In other words, it provide us with an object(the numpy N dimension array) where we can store our input data, labels and neural network parameters and easily do operations over them.\n",
    "\n",
    "### How is it useful?\n",
    "Since we can easily make operations over them we can take advantage of some matrix multiplication, addition and event apply a function over a numpy array. \n",
    "\n",
    "As a consequence instead of doing something like:\n",
    "\n",
    "```python\n",
    "    net_values = []\n",
    "\n",
    "    '''Iterating thorugh each of the categories'''\n",
    "    for category in range(0,10):\n",
    "        weighted_sum = 0\n",
    "        '''Calculating the weighted sum over all pixels on the image'''\n",
    "        for i,pixel in enumerate(image):\n",
    "            weighted_sum += weights[category][i]*pixel\n",
    "\n",
    "        '''Appending to the net value array'''\n",
    "        net_values.append(weighted_sum)\n",
    "```\n",
    "We can just take the dot product between the input and the weights:\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "weights_numpy = np.array(weights);\n",
    "image_numpy = np.array(image)\n",
    "\n",
    "'''Taking the dot product which is the same as the sum product \n",
    "of the weights with the pixels of an image'''\n",
    "a = np.dot(weights_numpy,image_numpy)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Lets try it. \n",
    "\n",
    "To prove how effective it is to use matrix multiplixation in numpy, lets do a simple experiment.\n",
    "\n",
    "Lets run the the same likes of code with a single image. \n",
    "\n",
    "\n",
    "[1]:http://www.numpy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Importing libraries'''\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cPickle, gzip\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "'''Reading the dataset'''\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "train_set_data = train_set[0][:10000]\n",
    "train_set_labels = train_set[1][:10000]\n",
    "\n",
    "test_set_data = test_set[0][:1000]\n",
    "test_set_labels = test_set[1][:1000]\n",
    "\n",
    "valid_set_data = valid_set[0][:500]\n",
    "valid_set_labels = valid_set[1][:500]\n",
    "\n",
    "# Adding bias term to train_set and test_set\n",
    "biases = np.array(([1] * 10000))\n",
    "train_set_data  = np.column_stack((train_set_data,biases))\n",
    "\n",
    "biases = np.array(([1] * 1000))\n",
    "test_set_data  = np.column_stack((test_set_data,biases))\n",
    "\n",
    "biases = np.array(([1] * 500))\n",
    "valid_set_data  = np.column_stack((valid_set_data,biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Initializing the weight array using numpy random'''\n",
    "categories = 10\n",
    "inputdimension = 785\n",
    "weigths = np.random.rand(categories, inputdimension)*0.0001\n",
    "image = train_set_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets see how long it takes to compute the net value per category in one single image.\n",
    "#### Without Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0054354590276208337, 0.0050760159385886917, 0.0053035633007418462, 0.0058384878686459545, 0.0053606548933386577, 0.0051109461617070816, 0.0057464526078418419, 0.004775540291094396, 0.0059407176692748351, 0.0050490441279610219]\n"
     ]
    }
   ],
   "source": [
    "net_values = []\n",
    "start_time = time.time()\n",
    "'''Iterating thorugh each of the categories'''\n",
    "for category in range(0,10):\n",
    "    weighted_sum = 0\n",
    "    '''Calculating the weighted sum over all pixels on the image'''\n",
    "    for i,pixel in enumerate(image):\n",
    "        weighted_sum += weigths[category][i]*pixel\n",
    "\n",
    "    '''Appending to the net value array'''\n",
    "    net_values.append(weighted_sum)\n",
    "end_time = time.time()\n",
    "\n",
    "print net_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed Without Numpy 0.00961589813232\n"
     ]
    }
   ],
   "source": [
    "time_elpased_NoNumpy = end_time -start_time\n",
    "print \"Time Elapsed Without Numpy\", time_elpased_NoNumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Woa, that was pretty fast now lets try it with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00543546  0.00507602  0.00530356  0.00583849  0.00536065  0.00511095\n",
      "  0.00574645  0.00477554  0.00594072  0.00504904]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "'''Taking the weighted sum, which is simply the dot product of the weights with the image'''\n",
    "a = np.dot(weigths,image)\n",
    "end_time = time.time()\n",
    "\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed With Numpy 0.000249862670898\n"
     ]
    }
   ],
   "source": [
    "time_elpased_Numpy = end_time -start_time\n",
    "print \"Time Elapsed With Numpy\", time_elpased_Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That was way faster lets check how many times faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.48473282442748"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_elpased_NoNumpy / time_elpased_Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see its more than an order of magitude faster than using a loop. This is due to the fact that numpy is optimized using C++ which makes it incredibly fast and efficient. The CPU is not looping though when we do the dot product it just does multiply and sum. \n",
    "\n",
    "Also, Numpy takes advantage of the all the cores in your computer which can run the computations in parallel. This is very important because it allow us to take adavantage of parallelism on multiple computers and even GPUs(Thanks Gamers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Lets build a two layered(DEEP) network on numpy. \n",
    "\n",
    "#### BackPropagation\n",
    "We have learned how to train the weights that exist between the input and the output using the **Learning Rule**. However, when dealing with more that one layer of weights we need to propagate the mistakes to train the weights on each layer. \n",
    "The layers that exist between the input and the output are called the the **Hidden Layers**.\n",
    "\n",
    "This sounds pretty complicated and in reality it is. It was until the 70s when the method of propagating the error back to all layers got popularized. \n",
    "\n",
    "We can write down the derivation, but its better if we just see the **formula**. j units are the hidden layer units and k units are the output layer units:\n",
    "\n",
    "$$ w_j(t+1) = w_j(t) + \\alpha * \\delta_j *x_j $$\n",
    "\n",
    "We can notice that the only thing that will be different is the delta, or the change of the weight.\n",
    "\n",
    "\n",
    "In the case of a hidden layer this delta would be different that the output delta since we do not have a teacher value of the correct result. \n",
    "In this formula $\\delta_j$ is defined as:\n",
    "$$\\delta_j = y'_k \\sum_{k}^{N} w_{jk}\\delta_k$$\n",
    "\n",
    "And $\\delta_k$ as:\n",
    "$$\\delta_k = (teacher_k - y_k )$$ For a particular k unit, or in this category.\n",
    "\n",
    "The parameters are defined as follow:\n",
    "\n",
    "$\\alpha$: Learning Rate\n",
    "\n",
    "$y'_k$: The derivative of the output value of unit k, where k is the category. Pretty much softmax'(net_k)\n",
    "\n",
    "$\\sum_{k}^{N} w_{jk}\\delta_k$: The sum of all the weights between the hidden and output layer mutiplied with their respective $\\delta$ errors.\n",
    "\n",
    "\n",
    "#### Why is this important. \n",
    "\n",
    "With the previous single layer neural network the weights leaned features about the pixels in the image. However, thse features were directly realted to the input value. There was no more to learn, which cause the neural network on fail in some cases. \n",
    "\n",
    "Now that we have more than one layer the neural network can learn more than input feautres. It is able to learn internal representations, which means that the weights in the layers in between the input and the output are feature that are recreated and learned by the neural network itself. \n",
    "\n",
    "Internal Representation its what have make neural networks so powerful.\n",
    "\n",
    "#### Lets go over the steps of updating the weights on the Hidden Layers\n",
    "\n",
    "* Step 1: Calculate the delta of the output layer\n",
    "* Step 2: Calculate the delta of the hidden layer using the output layer delta\n",
    "* Step 3: Update the hidden layer weights using the learning rule\n",
    "* Step 4: Update the output layer weights using the learning rule\n",
    "\n",
    "It is important to update the weights of the hidden layer first since they depend on the state of the output layer weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Lets Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the hot key category label arrays\n",
    "categoriesArray = []\n",
    "for label in train_set_labels:\n",
    "    tempArray = [0]*10\n",
    "    tempArray[label] = 1\n",
    "    categoriesArray.append(tempArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Start with the Functions\n",
    "#### Softmax or The Output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Needed in order to apply it to a vector'''\n",
    "def exponent(a):\n",
    "    return math.exp(a)\n",
    "'''Vectorizing the function in order to be applied to the vector'''\n",
    "exponentVecorized = np.vectorize(exponent)\n",
    "\n",
    "def softmax(a):\n",
    "    net_k = exponentVecorized(a)\n",
    "    denominator = np.sum(net_k)\n",
    "    net_k = net_k/denominator\n",
    "    return net_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The hidden layer activation function:\n",
    "In this case we are going to use a different activation function, in this case tanh or logistic regression works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanH(a):\n",
    "    op1 = exponent(a)\n",
    "    op2 = exponent(-a)\n",
    "    return ((op1 - op2)/(op1 + op2))\n",
    "\n",
    "'''The derivative'''\n",
    "def tanHPrime(a):\n",
    "    op1 = tanH(a)\n",
    "    return (1-(op1*op1))\n",
    "\n",
    "tanHVectorized = np.vectorize(tanH)\n",
    "tanHPrimeVectorized = np.vectorize(tanHPrime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining forward propagation using the dot product and the activation functions vertorized from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardPropagation(image, j_weigths, k_weights):\n",
    "\n",
    "    #Starting at the j layer\n",
    "    a_j = np.dot(j_weigths,image)\n",
    "    net_j = tanHVectorized(a_j)\n",
    "    net_j_prime = tanHPrimeVectorized(a_j)\n",
    "\n",
    "    #On the forward layer\n",
    "    a_k = np.dot(k_weights, net_j)\n",
    "    #Softmax.\n",
    "    net_k = softmax(a_k)\n",
    "    \n",
    "    return net_k, net_j, net_j_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backPropagation(image, net_k, net_jTuple , j_weigths, k_weights, index , learningRate):         \n",
    "    net_j = net_jTuple[0]\n",
    "    net_j_prime = net_jTuple[1]\n",
    "    #BackPropagation\n",
    "    #The output error\n",
    "    delta_k = (categoriesArray[index] - net_k)\n",
    "\n",
    "    #The hidden layer error\n",
    "    delta_j = net_j_prime * np.sum((k_weights.T*delta_k).T,axis=0)\n",
    "    \n",
    "    #Calculating the update rule by mutiplyng the deltas by their inputs, The last part of the learning rule\n",
    "    k_update = learningRate* np.outer(delta_k, net_j)  \n",
    "    \n",
    "    j_update = learningRate*np.outer(delta_j,image)  \n",
    "\n",
    "    #Updating the weights.\n",
    "    k_weights = k_weights + k_update  \n",
    "\n",
    "    j_weigths = j_weigths + j_update   \n",
    "    \n",
    "    return  j_weigths,k_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a testing function, we need to check the accuracy of our network at every epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testing(set_data, labels, j_weights,k_weights , activationFunction):\n",
    "    #Calculating error given the current weights at the epoch\n",
    "    correct = 0.0\n",
    "    #Iterating thorugh the data set\n",
    "    for index, digit in enumerate(set_data):\n",
    "        #Calculating the net output a over all the categories\n",
    "         #Forward propagation.\n",
    "            \n",
    "        #Starting at the j layer\n",
    "        a_j = np.dot(j_weights,digit)\n",
    "        net_j = activationFunction(a_j)\n",
    "\n",
    "        #On the forward layer\n",
    "        a_k = np.dot(k_weights, net_j)\n",
    "        net_k = softmax(a_k)\n",
    "        \n",
    "        #Obtaining the category with the max probability\n",
    "        maximum = 0.0\n",
    "        cat = 0\n",
    "        for i,out in enumerate(net_k):\n",
    "            if out > maximum:\n",
    "                maximum = out\n",
    "                cat = i\n",
    "        #Checking if classification was accurate\n",
    "        if(cat == labels[index] ):\n",
    "            correct+=1.0\n",
    "    #Calculating accuracy\n",
    "    accuracy = correct/len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets put it all together and run this network :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch:  0\n",
      "Train accuracy 0.9139 Validation accuracy 0.852\n",
      "At Epoch:  1\n",
      "Train accuracy 0.9245 Validation accuracy 0.89\n",
      "At Epoch:  2\n",
      "Train accuracy 0.9365 Validation accuracy 0.89\n",
      "At Epoch:  3\n",
      "Train accuracy 0.9328 Validation accuracy 0.876\n",
      "At Epoch:  4\n",
      "Train accuracy 0.9498 Validation accuracy 0.886\n",
      "At Epoch:  5\n",
      "Train accuracy 0.9593 Validation accuracy 0.916\n",
      "At Epoch:  6\n",
      "Train accuracy"
     ]
    }
   ],
   "source": [
    "'''This define the dimension of the hidden layer, play around with it and check the accuracy'''\n",
    "hiddenLayerSize = 50\n",
    "\n",
    "'''Do not touch these numbers since they define the input and the output'''\n",
    "imageSize = 785\n",
    "outLayerCategories = 10\n",
    "k_weights = np.random.rand(10,hiddenLayerSize) *0.0001\n",
    "j_weigths = np.random.rand(hiddenLayerSize,785)*0.0001\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    print \"At Epoch: \" , epoch\n",
    "\n",
    "    #Adding stochasticity to the data set(shuffl)\n",
    "    sample_indexes = np.arange(len(train_set_data))\n",
    "    np.random.shuffle(sample_indexes)\n",
    "\n",
    "    for index in sample_indexes:\n",
    "        image = train_set_data[index]\n",
    "        \n",
    "        '''Calculating the output val from propagation'''\n",
    "        retForward = forwardPropagation(image,j_weigths,k_weights)\n",
    "        \n",
    "        output = retForward[0]\n",
    "        net_j = retForward[1]\n",
    "        net_jPrime = retForward[2]\n",
    "        \n",
    "        net_jTuple = (net_j,net_jPrime)\n",
    "        '''Updating the weights'''\n",
    "        retWeight = backPropagation(image, output,net_jTuple , j_weigths,k_weights , index, learning_rate)\n",
    "        \n",
    "        '''Assigning the weights'''\n",
    "        j_weigths = retWeight[0]\n",
    "        k_weights = retWeight[1]\n",
    "        \n",
    "        \n",
    "    #Calculating error given the current weights at the epoch for all sets\n",
    "    valAccuracy =  testing(valid_set_data, valid_set_labels, j_weigths,k_weights , tanHVectorized)\n",
    "    trainAccuracy =  testing(train_set_data, train_set_labels, j_weigths,k_weights , tanHVectorized)\n",
    "\n",
    "    print \"Train accuracy\", trainAccuracy, \"Validation accuracy\", valAccuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WOW Such Accuracy:\n",
    "As you can see the performance of a single layer network was pretty good. This is due to the fact that the Neural Network was able to learn internal representations, which gives it more infomation to take decisions. \n",
    "\n",
    "Go to the next chapter in order to learn Neural Networks applied to **COMPUTER VISION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
