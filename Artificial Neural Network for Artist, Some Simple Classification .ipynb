{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasification\n",
    "\n",
    "### Classifying simple Images\n",
    "We leaned how we can train a neural network to learn the logic of an AND gate. Probably it was a good example but not as exiting. \n",
    "\n",
    "Aritifical Neural Networks are Universal Classifiers, this mean that if we are able to get information in a computable form and specify some classification classes we can pretty much train a neural network classify this infomration accurately.\n",
    "\n",
    "### The MNIST Dataset\n",
    "The MNIST Dataset is composed of 80 thousand digit images of 28 by 28 pixel resolution. The digits of these images are numbers from 0 to 9 handwritten by several people. Yann Le Cunn, the neural network god, used this dataset as a classification problem for zipcode numbers.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Importing libraries'''\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cPickle, gzip\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "'''Reading the dataset'''\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "\n",
    "train_set_data = train_set[0][:2000]\n",
    "train_set_labels = train_set[1][:2000]\n",
    "test_set_data = test_set[0][:500]\n",
    "test_set_labels = test_set[1][:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAEKCAYAAADZ1VPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFCpJREFUeJzt3X2wFfV9x/E3QaA8KdyigARBDSpGG+wkkAQjZ9AwGBOV\nGasRrfhQxrFobFIqYtNySdoEbaXGOj6NaAga1IZqRVsVrQeNCIxW8AmiBm54EC5XAQG9RpHbP76/\nc8/e5ZzfeX649/d5zZy5e/a3D7+7u5/d3+6ecxZERERERERERERERERERGrmUuCFIsf9b+Avy1eV\nmhgPvAPsBc7OY/iRwAHgCxWsk3RiTcDH2AaVet1aywplcCnFh74reBa4xlPeBEyMvB9J+UKfcNP6\nz1j/r7j+z0X6HQBeA7pF+v0TcF+Wen0RWAK0ALuB14FpwKmkt8V9bpzU+z1uvLgmbDveA+wCXgSu\njNXFJ163Ssl7PodUsBJtwHeB/63gPKQ0RwFvecrbyH/jLkYL8HWgAdjp+k0D3nbzjhoKfB9YHKlb\nNouAV7H/74/AnwFDgN8C/d0wI4CNwGFYWLKJbsf9sZ3VL4BxwOWe8eIquRwLmk+tmml3AL+JvL8R\neMZ1DwQeB3ZgG8JSYFhk2CTwU2yPuxd4DBgEPAB8CKzGVmjKAexo9ntsI7uJ7AvmBGAZ8AGwHvgL\nz/+QBK5w3Ze6+szHjgbvAt8ELgM2Ac3AJZFxz8I2yg9d+ZzYtC8B/gC8D/wYO9qc7sq6Ade7ebwP\nPIQts2ymY034D4D/wsIDtjyOwZbvHqBHbLxFWGiWYst5ZqTsYle/FuCGSP9C6/Yp8CgWZoDuwPnY\nuoyvo5uAuW6YXL4K/BJoxdb/GuDJ2DDFhHAvtjwuwHZOX3b9fevzefd3txt/HHAsthN5H1uG92M7\nn5RZwBZsvawn3dryLd9M86m6jaQ31LjewO+wBfct7B8/0pU1AFOAPwH6AQ8Dj0TGTWJHgqOBQ4E3\nsY16IrZBLATujQx/AGvGDgCGu/lGw5pq3vcFNrs6fQEY4+o1Osv/8BzpPf2lwGdu3G7YTmkL8O9Y\nmL6NrcA+bvgJpDeYk4HtwDnu/YnYSvumG/dfsHCkVvy1wApsefUA7gR+naWOE93/MAboiZ1eLY+U\nb6Rj8z0uXj4SW553Ab2wI+gnwPFF1C2BLe9vACtdv+9g4byCg5v3XwJeJr3ufM37ZdhR/QJsx5VJ\nfJxssi2jP2DNfPCvzxEZ5nMslo0e2AFrOfBvrux4bMcxxL0/Cts5g3/5ZppP1TVhG++uyOuKSPlY\n7EjehK2cbMaQbvqBbQyzI+//FXgi8v672F435QAwKfL+KtKtiktJh/4C0nvLlLuAf8xSr3jo346U\nnezme3ik3/tYSDK5BWsl4Ob3QKSsN9ZETW14b9FxIxyK7RQyrewFwLzI+75u2FQQig39kZF+q7Cj\nM8C6AuqWwEIPtuyOAx4ELiRz6I8BzsS2lx74Qz8A+DnwBrAf2x6+Gpt/fJxssi2jl+i4HUZF12c+\n8zkX+D/X/SWsZZjaKUT51n0+84F8BihBG7a3Gxh5LYiUrwY2uO7/iPTvg4WtCWsuLceaPtHmWHOk\n+xPsVCD6vl+sLpsj3ZvouNGmjMCaRNGd1FRgcIZhM4nWqdX9bYn1S9VrHLZR78CaY1cCf+rKjsRa\nCdHxPoi8H4m1fFJ1fAvbsDPVcyh2REr5yE1rWIZhC7E90v0x6f9rRAF1i1qEnYIl3PjZmt7/gy2b\nK/Gf0+/GAnmSm/ca7DSinIaRPhj51mcmg7Ed3BZsG18UGf5d4G+ARmybWkz6lGwkxS3fDmrZFJiB\nNTnfA66L9P9bbK8/Fgv7BGwjyLYh+FZ+ylGx7q0ZhtmE7WCiO6n+rp7l9mtsI/widlS6k/T/9x4d\nryL3puMGtAmYHKtnH2Bbhvm8h20oKX3dtDL9/5nks2yjCqlb1P1YC+wJbKft8/fYdYQ+OYZL+QC4\nGduZ+q4vFOJrWOh/695nWp+pbGVahj8DPsd2Sodht36jWVyMnfaOcOPf6Pr7lm/e66rSoc8W1OOw\n896LsItW12G3asCOGq3YHrCBgy9yxaebzwWZmaTP6X+AXQCJe8LV62KsWdUDW7kn5DH9QvXD9tSf\nYju3qZGyJcD3sHPdntgeP/o/3oltNKkd2eFkv8e+GLuY+BXsHPxn2Pnzpjzr2Yydf+arkLpFbQRO\nwwKdy3Ks2T7NM8yN2Dn2IdiO+yrsus+uPKafSWr5H4qdPi7Gjs5vuv6Z1mcqhC1Yszu6HPthra49\n2M7j7yJlx2FN+F7Yad0n2A4C/Ms303wyqnToU1d+U68l2MW2Rdi55utYc+YG168Hdj7UGzsHXoE1\n6eJ7sbZYt68c7Kr1K9i53eOkTzOi4+7Fzv2/jx0Jt2HnhT3z+D/zqUPUXwM/wVb6P9BxJ/Qm1tR9\nEDtS78WajX905b/A7lg87cZ/CdvQMnnWTX+Jm9bRpK+U5+Pn2N2DXcCPXD/f/1VI3eLTWkH6tCG+\nPOPz/DF2QMg2TG/SzeDfYzv7TDuffI+OqTscm7DThpuxnWmKb31+DPwzdndnJ7Y85gJ/jh3YlmLr\nJ1WXXthyb8G2wUGkrx34lm90PrvwL/cuL3URqLPqh90ZGJFrQBExnTH038PO1fpiTbpXalsd6UpC\n+Ax1oRej6sHZ2CnGVuwcrZAmuYiISHlMxj4i+A72sUER6cK6Y1fdR2JX3NcQ+7jqhAkTUldg9dJL\nryq/XP7K6ht0/ALD9e4V1RY1Z86ctnqm+pVG9StNuevnwp9RsRfyhtHxo61bKP2jnSJSBcWGvuxN\nBxGpjmJ/RGMr9imnlOF0/JIIAI2Nje3dAwYMKHJW1ZFIJGpdBS/VrzRdvX7JZJJkMpnXsMX+msch\n2PfST8c+3rka+0rkusgw7tRCRKqtW7dukCXfxR7p9wNXA09hV/IX0DHwIlKnKvm7XTrSi9SI70gf\nwsdwRSRCoRcJjEIvEhiFXiQwCr1IYBR6kcAo9CKBUehFAqPQiwRGoRcJjEIvEhiFXiQwCr1IYBR6\nkcAo9CKBUehFAqPQiwRGoRcJjEIvEhiFXiQwCr1IYBR6kcAo9CKBKfZhF1Jjzc3N3vKnnnrKWz5v\n3jxv+cSJE3PWYezYsTmH8bnooou85d27dy9p+pKZjvQigVHoRQKj0IsERqEXCYxCLxIYhV4kMAq9\nSGBKfT59E7AH+Bz4DIjeuNXz6Uvw+OOPe8unTp3qLd+7d285q1MR69at85afcMIJVapJ1+N7Pn2p\nH85pAxLAzhKnIyJVUo7mfamtBRGpolJD3wY8A7wMTC+9OiJSaaU278cD24DDgWXAeuCFVGFjY2P7\ngIlEgkQiUeLsRCSTZDJJMpnMa9hyNs3nAPuAm917XcgrgS7k6UJeKXwX8kpp3vcB+rvuvsAk4PUS\npiciVVBK834w8EhkOg8AT5dcIxGpqEpeeVfzvgStra3e8mOPPdZbvm3btnJWpyIaGhq85cuXL/eW\nn3TSSeWsTpdSqea9iHRCCr1IYBR6kcAo9CKBUehFAqPQiwRGoRcJjH73vk717t3bW37XXXd5yy+8\n8EJv+UcffeQtP+aYY7zlABs2bMg5jM/Onf5vZC9dutRbrvv0xdGRXiQwCr1IYBR6kcAo9CKBUehF\nAqPQiwRGoRcJjL5P30WNHz/eW75ixQpveT7Pnl+9enVBdSpUrvv4AwcOrOj8OzN9n15E2in0IoFR\n6EUCo9CLBEahFwmMQi8SGIVeJDC6T99FrVy50ls+c+ZMb/mLL75YzuoUpbm52Vt+xBFHVKkmnY/u\n04tIO4VeJDAKvUhgFHqRwCj0IoFR6EUCo9CLBCaf+/T3AmcBO4CTXb8G4CFgBNAEnA/sjo2n+/R1\nbN++fd7yM844I+c0Vq1aVa7qZDR9+nRv+d13313R+Xdmpd6nvw+YHOt3PbAMOA541r0XkU4gn9C/\nAOyK9TsbWOi6FwLnlrNSIlI5xZ7TDwZSn5Fsdu9FpBMox7Ps2tzrII2Nje3diUSCRCJRhtmJSFwy\nmSSZTOY1bLGhbwaGANuBodhFvoNEQy8ilRM/qM6dOzfrsMU27x8DprnuacCjRU5HRKosn9AvBlYA\nxwObgcuAecC3gbeBie69iHQC+TTvsz3oPPeNXKmZ559/3lue6x57pX/TPh+nn356ravQJekTeSKB\nUehFAqPQiwRGoRcJjEIvEhiFXiQwCr1IYPS793WqpaXFWz5p0iRv+RtvvOEt379/f8F1qjY9n754\n+t17EWmn0IsERqEXCYxCLxIYhV4kMAq9SGAUepHAlOM38qQCNm7c6C1fv369t7wz3IfP5dZbb/WW\nz5kzp0o16Vp0pBcJjEIvEhiFXiQwCr1IYBR6kcAo9CKBUehFAqP79HVq7Nix3vJFixZ5yy+55BJv\neWtra8F1qratW7fWugpdko70IoFR6EUCo9CLBEahFwmMQi8SGIVeJDAKvUhg8rlPfy9wFrADONn1\nawT+Ckj9OPts4MlyV06yO++887zlo0aN8pbv2bOn5Dp8/vnn3vIpU6Z4y3fv3l1yHaRw+Rzp7wMm\nx/q1AfOBU9xLgRfpJPIJ/QvArgz9K/l0HBGpkFLO6a8B1gILgAHlqY6IVFqxn72/A/iJ6/4pcDNw\nRXygxsbG9u5EIkEikShydiLik0wmSSaTeQ1bbOh3RLrvAZZmGigaehGpnPhBde7cuVmHLbZ5PzTS\nPQV4vcjpiEiV5XOkXwxMAAYBm4E5QAIYg13F3whcWaH6iUiZ6fn0UrRc6/f222/3ll999dXe8tGj\nR3vLX3rpJW/5YYcd5i3vyvR8ehFpp9CLBEahFwmMQi8SGIVeJDAKvUhgFHqRwOh376Voub5Pn+s+\nfC69evXylrt70VIgHelFAqPQiwRGoRcJjEIvEhiFXiQwCr1IYBR6kcDoPr0Ubf78+RWd/syZM73l\nhx56aEXn31XpSC8SGIVeJDAKvUhgFHqRwCj0IoFR6EUCo9CLBEa/e59Fa2urt/yqq67yll9++eXe\n8tNOO63gOlXTvn37cg4zfPhwb3mpz5/fuXOnt3zgwIElTb8r0+/ei0g7hV4kMAq9SGAUepHAKPQi\ngVHoRQKj0IsEJtf36YcDvwKOANqAu4FbgQbgIWAE0AScD5R2U7bOzJo1y1u+cOFCb/maNWu85Q8/\n/LC3fNCgQd7yhoYGb/nmzZu95U1NTd7y2bNne8uh9Pvw8+bN85b379+/pOlLZrmO9J8BPwS+DHwd\nmAGMBq4HlgHHAc+69yLSCeQK/XYgdcjaB6wDhgFnA6lD3ULg3IrUTkTKrpBz+pHAKcAqYDDQ7Po3\nu/ci0gnk+xt5/YAlwLXA3lhZm3sdpLGxsb07kUiQSCQKrqCI5JZMJkkmk3kNm0/oe2CBXwQ86vo1\nA0Ow5v9QYEemEaOhF5HKiR9U586dm3XYXM37bsAC4C3glkj/x4Bprnsa6Z2BiNS5XEf68cDFwGvA\nq67fbGAe8DBwBelbdiLSCej79Fls2LDBWz5jxgxv+ZNPPlnS/EeNGuUtHzdunLd86dKl3vIPP/yw\n4DrF5Xo+/JgxY7zlK1eu9Jb37Nmz4DqJ0ffpRaSdQi8SGIVeJDAKvUhgFHqRwCj0IoFR6EUCo/v0\nRcr1XfATTzzRW37OOeeUszo1kes7/y0tLVWqicTpPr2ItFPoRQKj0IsERqEXCYxCLxIYhV4kMAq9\nSGB0n75C9u/f7y1fvHhxSdNfvXq1t/y2224rafr5PPt97dq13vJcz6+XytF9ehFpp9CLBEahFwmM\nQi8SGIVeJDAKvUhgFHqRwOg+vUgXpPv0ItJOoRcJjEIvEhiFXiQwCr1IYBR6kcDkCv1w4DngTeAN\n4AeufyOwBXtm/avA5ArVT0TKLNd9+iHutQboB7wCnAucD+wF5nvG1X16kRrx3ac/JMe4290LYB+w\nDhiWmm45Kici1VXIOf1I4BRgpXt/DbAWWAAMKG+1RKRS8g19P+A3wLXYEf8O4GhgDLANuLkitROR\nssvVvAfoASwB7gcedf12RMrvAZZmGrGxsbG9O5FIkEgkiqmjiOSQTCZJJpN5DZvrvLwbsBD4APhh\npP9Q7AiP6/81YGpsXF3IE6kR34W8XKE/FXgeeA1IJfgG4EKsad8GbASuBJpj4yr0IjVSSuhLodCL\n1Ii+Wisi7RR6kcAo9CKBUehFAqPQiwRGoRcJjEIvEhiFXiQwCr1IYBR6kcAo9CKBUehFAlO10Of7\nXd9aUf1Ko/qVppr1U+gd1a80ql9pumToRaQ+KPQigankj2gkgQkVnL6IZLccSNS6EiIiIiIi0iVM\nBtYD7wCzalyXTJqwX/x9FVhd26oAcC/268KvR/o1AMuAt4Gnqe1ThTLVr5H6eKhptoeu1svyC+Kh\nsN2Bd7HHYvXAHoY5upYVymAjtlHUi29hjxCLhuom4DrXPQuYV+1KRWSq3xzgR7WpTgdDsJ9nB3sy\n0++w7a1ell+2+lVt+VXjlt1YLPRNwGfAg8A5VZhvoerpgZwvALti/c7GHjyC+3tuVWvUUab6QX0s\nw+3YgQU6PnS1XpZftvpBlZZfNUI/DNgceb+F9D9ZL9qAZ4CXgek1rks2g0k/UKTZva839fZQ05FY\ni2QV9bn8RlKDh8JWI/Sd4YkX47GFfyYwA2u+1rM26m+51ttDTfthz2C8FtgbK6uH5Vezh8JWI/Rb\nsYsXKcOxo309ST2XrwV4BDslqTfN2Pkg2LMEd3iGrYUdpMN0D7VdhqmHri4i/dDVelp+2R4KW5Xl\nV43QvwyMwpoyPYELgMeqMN989QH6u+6+wCQ6XqCqF48B01z3NNIbS70YGumeQu2WYTesefwWcEuk\nf70sv2z1q5flVzZnYlcp3wVm17gucUdjF1bWYLdQ6qF+i4H3gE+x6yGXYXcXnqH2t5zg4PpdDvwK\nu+25FgtUrc6ZTwUOYOszevurXpZfpvqdSf0sPxERERERERERERERERERERERKdb/A/mRDKa9dSzC\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102d94bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Reformating the image array into a matrix'''\n",
    "counter = 0\n",
    "imgArray  = []\n",
    "row = []\n",
    "for i in train_set_data[10]:\n",
    "    if(counter%28 == 0):\n",
    "        if counter !=0 : \n",
    "            imgArray.append(row)\n",
    "        row = []\n",
    "    row.append(i)\n",
    "    counter+=1\n",
    "imgArray.append(row)\n",
    "plt.title(\"Example image of the MNIST Dataset\")\n",
    "digit = plt.imshow(imgArray, cmap='Greys',  interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Classificaiton Problem\n",
    "On the previous lesson we used a single neuron since we only cared about a single output. In the case of classification we need a neuron per class or in this case per digit. The neuron that will have the highest output value will be the one that determines the correct classification. \n",
    "\n",
    "As an example, in this classification problem we will have 10 neurons each taking the product sum (sum of weight*pixel vale) of the pixel values with their respective weight value into its activation function. \n",
    "\n",
    "#### Single Neuron example\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/perceptron_node.png \"Logo Title Text 1\")\n",
    "\n",
    "In this case, if we want to classify the number 6 in our images the sixth output neuron will have a higuer value with respect to the other values. If we think in terms of probability we will have a probability distribution over each of the classes and the output with the higuest probability will determine the classification.\n",
    "\n",
    "### Softmax for classification\n",
    "We are going to built and train a neural network to classify an image into a digit, but first we need a smarter activation function. \n",
    "\n",
    "Softmax is a function that can give us the probability of a class given the output of all the neurons. In other, it is a function that creates a probability distribution among all classes. \n",
    "\n",
    "#### Softmax:\n",
    "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/001ce4c2c74e78a66a4d7d04ab92cbd0d0fdec02 \"Logo Title Text 1\")\n",
    "\n",
    "In this case j is the category that we want to get the probability of given the input x, which is multiplied by its appropiate weights. \n",
    "\n",
    "We do not have to go deep into the meaning of the function we just need to know that it is the best activation function for classification. \n",
    "\n",
    "### Learning rule\n",
    "The learning rule for softmax focuses on reducing an error function. The way to reduce the error is to move the weight parameters in the direction of the descreasing rate of change in order to archieve a minimum error. \n",
    "\n",
    "For softmax the error function is Cross Entropy. For practical puposes we will just describe the derived learning rule for softmax. You can learn more about this function and the derivation as a learning rule online.\n",
    "\n",
    "The learning rule for the weights is defined as:\n",
    "\n",
    "$$ w_{ik}(t+1) = w_{ik}(t) + \\alpha * (teacher - output) *x_i $$\n",
    "\n",
    "This is for a particular weight from input i to category or neuron k.  \n",
    "\n",
    "Does it look familiar?\n",
    "\n",
    "Yes, it is the same as the perceptron. For some reason the math works out giving us an universal learning rule that can be applied to multiple activation functions(softmax as an example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets jump into an example aka Gradiend Descent. \n",
    "Lets build a neural network that takes all the pixel as inputs, they get multiplied by their respective weights and plug in into their respective activation functions. \n",
    "\n",
    "In Neural Networks we defined the input layer as the point where the infomation gets passed in. In this case, our input layer will be all the pixels going into the network. \n",
    "\n",
    "The output layer is defined, as the result of the neural nework operation. In this case, we will have an output layer consiting of an array of size 10 which defines the probability of each digit. The maximum value on that array defines the correct classification. \n",
    "\n",
    "![alt text](https://usercontent1.hubstatic.com/3993124_f260.jpg \"Logo Title Text 1\")\n",
    "\n",
    "In the case of this problem we will have each pixel as an input which will have an appropiate weight to an output. The total number of weights will be (25 \\* 25)+1 [pixels] * 10[categories]. We add an extra input as the bias term.\n",
    "\n",
    "In this case the teacher will be 0 or one depending which category k we are trying to learn. \n",
    "The output will be a a probability value from 0-1.\n",
    "\n",
    "To build the neural network we have to do these simple steps:\n",
    "\n",
    "1. Get the input pixel vector multiply it by their respective category weights.\n",
    "2. Plug in the result into the softmax equation.\n",
    "3. Calculate the delta(error) teacher-output.\n",
    "4. Update the weights using the learning rule. \n",
    "5. Repeat for every example.\n",
    "\n",
    "Steps 1 and 2 are called **Forward Propagation**, since we are propagating the inputs into the outputs. \n",
    "\n",
    "Steps 3 and 4 are the **Learning** steps. \n",
    "\n",
    "train_set_data contains an array of pixels and  train_set_labels contains its appropiate labeled classification. \n",
    "Here is an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image array length 784 Classigication lavel:  5\n"
     ]
    }
   ],
   "source": [
    "print \"Image array length\", len(train_set_data[0]), \"Classigication lavel: \", train_set_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets build the neural network :\n",
    "### Lets start by adding the bias term to each image array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Initializing the new image array'''\n",
    "new_train_set_date = []\n",
    "for image in train_set_data:\n",
    "    '''Appending the bias value 1'''\n",
    "    image = np.append(image,1)\n",
    "    '''appending the new image to the new image array'''\n",
    "    new_train_set_date.append(image)\n",
    "\n",
    "    \n",
    "    \n",
    "new_test_set_date = []\n",
    "for image in test_set_data:\n",
    "    '''Appending the bias value 1'''\n",
    "    image = np.append(image,1)\n",
    "    '''appending the new image to the new image array'''\n",
    "    new_test_set_date.append(image)\n",
    "    \n",
    "    \n",
    "    \n",
    "'''Updating the old array witht the new one'''\n",
    "train_set_data = new_train_set_date\n",
    "test_set_data = new_test_set_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Its time to initialize the weights to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Defined dimensions'''\n",
    "ncols = 785\n",
    "nrows = 10\n",
    "\n",
    "'''Use python syntax magic to create a matrix'''\n",
    "weights = [[0] * ncols for i in range(nrows)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert label value into a hot encoding array. \n",
    "This means that for each image we create an array of 10 categories. Only the index of the correct category has a 1 the other get 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoding = []\n",
    "for label in train_set_labels:\n",
    "    encode = [0]*10\n",
    "    encode[label] = 1\n",
    "    label_encoding.append(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the softmax function with 10 categories (k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(net_values):\n",
    "    '''Softmax function denominator'''\n",
    "    denominator = 0\n",
    "    \n",
    "    exp_values = []\n",
    "    '''Adding all the exponent of the net values together and saving their value'''\n",
    "    for val in net_values:\n",
    "        exponent = math.exp(val)\n",
    "        \n",
    "        exp_values.append( exponent)\n",
    "        denominator += exponent\n",
    "    \n",
    "    '''Calculating the softmax value by doing the division'''\n",
    "    softmax_array = []\n",
    "    for val in exp_values:\n",
    "        softmax_array.append(val/denominator)\n",
    "    \n",
    "    '''Returning the result'''\n",
    "    return softmax_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets code the first two steps **Forward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forwardPropagation(image,weights):\n",
    "    net_values = []\n",
    "\n",
    "    '''Iterating thorugh each of the categories'''\n",
    "    for category in range(0,10):\n",
    "\n",
    "        weighted_sum = 0\n",
    "        '''Calculating the weighted sum over all pixels on the image'''\n",
    "        for i,pixel in enumerate(image):\n",
    "            weighted_sum += weights[category][i]*pixel\n",
    "\n",
    "        '''Appending to the net value array'''\n",
    "        net_values.append(weighted_sum)\n",
    "\n",
    "    '''Calculating the neural network output'''\n",
    "    output = softmax(net_values)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets make that neural network learn, Step 3 and 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning(image, weights, image_index, alpha):\n",
    "    '''Getting the deltas'''\n",
    "    deltas = []\n",
    "    for cat, out_val in enumerate(output):\n",
    "        deltas.append(label_encoding[image_index][cat] - out_val)\n",
    "\n",
    "    '''Update the weights'''\n",
    "    for category in range(0,10):\n",
    "        '''Calculating the weighted sum over all pixels on the image'''\n",
    "        for i,pixel in enumerate(image):\n",
    "            weights[category][i] = weights[category][i] + alpha*deltas[category]*pixel\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to iterate over the dataset and train our network \n",
    "An epoch is defined to be a complete look over all the training dataset. \n",
    "We need to go over the whole dataset more that once since we might not be able to train our neural network enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Number: \n",
      "100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 Done\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 1\n",
    "alpha = 0.01\n",
    "\n",
    "image_counter = 0\n",
    "''''''\n",
    "for epoch in range(number_of_epochs):\n",
    "    '''Iterating throught the data set'''\n",
    "    print \"Image Number: \"\n",
    "    for index,image in enumerate(train_set_data):\n",
    "        \n",
    "        if(image_counter == 100):\n",
    "            image_counter = 0\n",
    "            print index,\n",
    "            \n",
    "        output = forwardPropagation(image,weights)\n",
    "        \n",
    "        weights = learning(image, weights, index,alpha)\n",
    "        \n",
    "        image_counter+=1\n",
    "    print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy\n",
    "Accuracy can be defined by the number of correct classificatoin over the total number of samples. \n",
    "\n",
    "We can do this by simply running forward propagation over the testing set and counting if it was classified correctly.\n",
    "\n",
    "Lets code a accuracy check function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracyCheck(weights, dataset, labels):\n",
    "    '''Initialize the number of correct values'''\n",
    "    correct = 0\n",
    "    \n",
    "    '''Iterate through all the images'''\n",
    "    for index,image in enumerate(dataset):\n",
    "        \n",
    "        '''Applying forward propagation on the images'''\n",
    "        output = forwardPropagation(image,weights)\n",
    "        \n",
    "        '''Getting the max val from the output'''\n",
    "        max_val = 0\n",
    "        max_index = -1\n",
    "        for i, out_val in enumerate(output):\n",
    "            if out_val > max_val:\n",
    "                max_val = out_val\n",
    "                max_index = i\n",
    "        \n",
    "        '''Comparing to the correct label, and update the correct count'''\n",
    "        if max_index == labels[index]:\n",
    "            correct+=1\n",
    "    '''Print the accuracy #correct/total'''\n",
    "    print (correct*1.0/len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is useful to undestand how good our neural network is at modeling data. Specially data that it have not seen before. \n",
    "This is why we check the accuracy on a testing set, which is data that the network have not seen before and training set which is the data that it already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.8955\n",
      "Test Accuracy:  0.848\n"
     ]
    }
   ],
   "source": [
    "'''Print out the accuracy values of train set and test set.'''\n",
    "print \"Train Accuracy: \",\n",
    "accuracyCheck(weights, train_set_data, train_set_labels)\n",
    "print \"Test Accuracy: \",\n",
    "accuracyCheck(weights, test_set_data, test_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
